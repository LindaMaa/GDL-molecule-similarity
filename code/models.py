# -*- coding: utf-8 -*-
"""models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AoPMtl4QzVzcCUxvn5114nSFUnFCKiZV
"""

class GraphAttention(nn.Module):
    def __init__(self, in_channels, out_channels, dropout=0.5):
        super(GraphAttention, self).__init__()
        self.attention = GATConv(in_channels, out_channels)
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = F.dropout(x, p=self.dropout, training=self.training)
        return self.attention(x, edge_index)

class GraphAttentionModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):
        super(GraphAttentionModel, self).__init__()
        self.attention1 = GraphAttention(in_channels, hidden_channels, dropout=dropout)
        self.attention2 = GraphAttention(hidden_channels, hidden_channels, dropout=dropout)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.attention1(x, edge_index)
        x = F.relu(x)
        x = self.attention2(x, edge_index)
        return global_mean_pool(x, data.batch)


class SiameseGraphAttentionModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, dropout=0.5):
        super(SiameseGraphAttentionModel, self).__init__()
        self.embedding = GraphAttentionModel(input_dim, hidden_dim, hidden_dim, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(p=dropout),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, data1, data2):
        embedding1 = self.embedding(data1)
        embedding2 = self.embedding(data2)
        combined = torch.cat((embedding1, embedding2), dim=-1)
        similarity = torch.sigmoid(self.fc(combined))
        return similarity.unsqueeze(-1) 

class GCNLayer(torch.nn.Module):
    def __init__(self, in_channels, out_channels, dropout=0.3):
        super(GCNLayer, self).__init__()
        self.conv = GCNConv(in_channels, out_channels)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index):
        x = self.conv(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        return x

class GraphGCNModel(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.3):
        super(GraphGCNModel, self).__init__()
        self.gcn1 = GCNLayer(in_channels, hidden_channels, dropout=dropout)
        self.gcn2 = GCNLayer(hidden_channels, out_channels, dropout=dropout)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.gcn1(x, edge_index)
        x = self.gcn2(x, edge_index)
        return global_mean_pool(x, data.batch)

class SiameseGraphGCNModel(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, dropout=0.3):
        super(SiameseGraphGCNModel, self).__init__()
        self.embedding = GraphGCNModel(input_dim, hidden_dim, hidden_dim, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(p=dropout),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, data1, data2):
        embedding1 = self.embedding(data1)
        embedding2 = self.embedding(data2)
        combined = torch.cat((embedding1, embedding2), dim=-1)
        similarity = torch.sigmoid(self.fc(combined))
        return similarity.unsqueeze(-1)  

class GraphGIN(nn.Module):
    def __init__(self, in_channels, out_channels, dropout=0):
        super(GraphGIN, self).__init__()
        self.gin = GINConv(nn.Sequential(nn.Linear(in_channels, out_channels),
                                          nn.ReLU(),
                                          nn.Linear(out_channels, out_channels)))
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = F.dropout(x, p=self.dropout, training=self.training)
        return self.gin(x, edge_index)


class GraphGINModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0):
        super(GraphGINModel, self).__init__()
        self.gin1 = GraphGIN(in_channels, hidden_channels, dropout=dropout)
        self.gin2 = GraphGIN(hidden_channels, hidden_channels, dropout=dropout)
   

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.gin1(x, edge_index)
        x = F.relu(x)
        x = self.gin2(x, edge_index)
        return global_mean_pool(x, data.batch)


class SiameseGraphGINModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, dropout=0):
        super(SiameseGraphGINModel, self).__init__()
        self.embedding = GraphGINModel(input_dim, hidden_dim, hidden_dim, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(p=dropout),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, data1, data2):
        embedding1 = self.embedding(data1)
        embedding2 = self.embedding(data2)
        combined = torch.cat((embedding1, embedding2), dim=-1)
        similarity = torch.sigmoid(self.fc(combined))
        return similarity.unsqueeze(-1) 

class ChebNetLayer(nn.Module):
    def __init__(self, in_channels, out_channels, K=4, dropout=0.5):
        super(ChebNetLayer, self).__init__()
        self.cheb = ChebConv(in_channels, out_channels, K)
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = F.dropout(x, p=self.dropout, training=self.training)
        return self.cheb(x, edge_index)

class ChebNetModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, K, dropout=0.5):
        super(ChebNetModel, self).__init__()
        self.cheb1 = ChebNetLayer(in_channels, hidden_channels, K, dropout=dropout)
        self.cheb2 = ChebNetLayer(hidden_channels, hidden_channels, K, dropout=dropout)
        self.cheb3 = ChebNetLayer(hidden_channels, out_channels, K, dropout=dropout)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.cheb1(x, edge_index)
        x = F.relu(x)
        x = self.cheb2(x, edge_index)
        x = F.relu(x)
        x = self.cheb3(x, edge_index)
        return global_mean_pool(x, data.batch)

class SiameseChebNetModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, K, dropout=0.5):
        super(SiameseChebNetModel, self).__init__()
        self.embedding = ChebNetModel(input_dim, hidden_dim, hidden_dim, K, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(p=dropout),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, data1, data2):
        embedding1 = self.embedding(data1)
        embedding2 = self.embedding(data2)
        combined = torch.cat((embedding1, embedding2), dim=-1)
        similarity = torch.sigmoid(self.fc(combined))
        return similarity.unsqueeze(-1) 
        
class CNNModel(nn.Module):
    def __init__(self, fingerprint_size, hidden_size, output_size, dropout=0.5):
        super(CNNModel, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv1d(1, hidden_size, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2),
            nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2),
        )
        self.fc = nn.Linear(hidden_size * (fingerprint_size // 4), output_size)

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.cnn(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

class SiameseCNNModel(nn.Module):
    def __init__(self, fingerprint_size, hidden_size, dropout=0.5):
        super(SiameseCNNModel, self).__init__()
        self.embedding = CNNModel(fingerprint_size, hidden_size, hidden_size, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(2 * hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(p=dropout),
            nn.Linear(hidden_size, 1)
        )

    def forward(self, fp1, fp2):
        embedding1 = self.embedding(fp1)
        embedding2 = self.embedding(fp2)
        combined = torch.cat((embedding1, embedding2), dim=-1)
        similarity = torch.sigmoid(self.fc(combined))
        return similarity.unsqueeze(-1)


class TransformerLayer(nn.Module):
    def __init__(self, in_channels, out_channels, dropout=0.5):
        super(TransformerLayer, self).__init__()
        self.transformer = TransformerConv(
            in_channels,
            out_channels,
            heads=1,
            edge_dim=None,
            dropout=dropout,
        )
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = F.dropout(x, p=self.dropout, training=self.training)
        return self.transformer(x, edge_index)


class TransformerModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):
        super(TransformerModel, self).__init__()
        self.transformer1 = TransformerLayer(in_channels, hidden_channels, dropout=dropout)
        self.transformer2 = TransformerLayer(hidden_channels, out_channels, dropout=dropout)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.transformer1(x, edge_index)
        x = F.relu(x)
        x = self.transformer2(x, edge_index)
        return global_mean_pool(x, data.batch)


class SiameseTransformerModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, dropout=0.5):
        super(SiameseTransformerModel, self).__init__()
        self.embedding = TransformerModel(input_dim, hidden_dim, hidden_dim, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(p=dropout),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, data1, data2):
        embedding1 = self.embedding(data1)
        embedding2 = self.embedding(data2)
        combined = torch.cat((embedding1, embedding2), dim=-1)
        similarity = torch.sigmoid(self.fc(combined))
        return similarity.unsqueeze(-1)

# NOTE THIS CODE IS ADAPTED FROM GDL LABS
class MPNNLayer(MessagePassing):
    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):
        super().__init__(aggr=aggr)
        self.emb_dim = emb_dim
        self.edge_dim = edge_dim    
        self.mlp_msg = Sequential(
            Linear(2*emb_dim + edge_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(),
            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()
          )
        self.mlp_upd = Sequential(
            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), 
            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()
          )

    def forward(self, h, edge_index, edge_attr):
        return self.propagate(edge_index, h=h, edge_attr=edge_attr)

    def message(self, h_i, h_j, edge_attr):
        msg = torch.cat([h_i, h_j, edge_attr], dim=-1)
        return self.mlp_msg(msg)
    
    def aggregate(self, inputs, index):
        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)
    
    def update(self, aggr_out, h):
        upd_out = torch.cat([h, aggr_out], dim=-1)
        return self.mlp_upd(upd_out)

    def __repr__(self) -> str:
        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')

    
# credits: adapted from GDL labs
class MPNNModel(Module):
    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):

        super().__init__()
        self.lin_in = Linear(in_dim, emb_dim)
        self.convs = torch.nn.ModuleList()
        for layer in range(num_layers):
            self.convs.append(MPNNLayer(emb_dim, edge_dim, aggr='add'))
        self.pool = global_mean_pool
        self.lin_pred = Linear(emb_dim, out_dim)
        
    def forward(self, data):
        h = self.lin_in(data.x) 
        for conv in self.convs:
            h = h + conv(h, data.edge_index, data.edge_attr) 


        h_graph = self.pool(h, data.batch) 
        out = self.lin_pred(h_graph)
        return out.view(-1)

    
# credits: adapted from GDL labs
class InvariantMPNNLayer(MessagePassing):
    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):
        super().__init__(aggr=aggr)
        self.emb_dim = emb_dim
        self.edge_dim = edge_dim
        self.mlp_msg = Sequential(
            Linear(2*emb_dim + edge_dim + 1, emb_dim), BatchNorm1d(emb_dim), ReLU(),
            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()
          )
        self.mlp_upd = Sequential(
            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), 
            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()
          )
        

    def forward(self, h, pos, edge_index, edge_attr):

        return self.propagate(edge_index, h=h,pos=pos,edge_attr=edge_attr)

    def message(self, h_i, h_j, pos_i, pos_j, edge_attr):
      dist = ((pos_i - pos_j).pow(2).sqrt()) 
      dist = torch.norm(dist,dim=-1)
      dist=dist.reshape(-1,1)
      msg = torch.cat([h_i, h_j, edge_attr,dist], dim=-1)
      return self.mlp_msg(msg)

    
    def aggregate(self, inputs, index):
        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)
    
    def update(self, aggr_out, h):
        upd_out = torch.cat([h, aggr_out], dim=-1)
        return self.mlp_upd(upd_out)

    def __repr__(self) -> str:
        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')

# credits: adapted from GDL labs
class InvariantMPNNModel(MPNNModel):
    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):
        super().__init__()
        self.lin_in = Linear(in_dim, emb_dim)
        self.convs = torch.nn.ModuleList()
        for layer in range(num_layers):
            self.convs.append(InvariantMPNNLayer(emb_dim, edge_dim, aggr='add'))
        self.pool = global_mean_pool
        self.lin_pred = Linear(emb_dim, out_dim)
        
    def forward(self, data):
        h = self.lin_in(data.x) 
        for conv in self.convs:
            h = h + conv(h, data.pos, data.edge_index, data.edge_attr)  
        h_graph = self.pool(h, data.batch) 
        return h_graph

class SiameseInvariantMPNNModel(nn.Module):
    def __init__(self, num_layers=2, emb_dim=64, in_dim=11, edge_dim=4, dropout=0.5):
        super(SiameseInvariantMPNNModel, self).__init__()
        self.embedding = InvariantMPNNModel(num_layers, emb_dim, in_dim, edge_dim)
        self.fc = nn.Sequential(
            nn.Linear(2 * emb_dim, emb_dim),
            nn.ReLU(),
            nn.Dropout(p=dropout),
            nn.Linear(emb_dim, 1)
        )

    def forward(self, data1, data2):
        embedding1 = self.embedding(data1)
        embedding2 = self.embedding(data2)
        combined = torch.cat((embedding1, embedding2), dim=-1)
        similarity =  torch.sigmoid(self.fc(combined))
        return similarity.unsqueeze(-1)

class EquivariantMPNNLayer(InvariantMPNNLayer):
    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):
        super().__init__(emb_dim=emb_dim, edge_dim=edge_dim, aggr=aggr)

    def forward(self, h, pos, edge_index, edge_attr):
        out = self.propagate(edge_index, h=h, pos=pos, edge_attr=edge_attr)
        return out

    def message(self, h_i, h_j, pos_i, pos_j, edge_attr):
        dist = (pos_i - pos_j).pow(2).sum(dim=-1).sqrt().unsqueeze(-1)
        msg = torch.cat([h_i, h_j, edge_attr, dist], dim=-1)
        return self.mlp_msg(msg)

    def update(self, aggr_out, h, pos):
        upd_out = torch.cat([h, aggr_out], dim=-1)
        return self.mlp_upd(upd_out)

class EquivariantMPNNModel(InvariantMPNNModel):
    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):
        super().__init__(num_layers=num_layers, emb_dim=emb_dim, in_dim=in_dim, edge_dim=edge_dim, out_dim=out_dim)

        self.convs = torch.nn.ModuleList()
        for layer in range(num_layers):
            self.convs.append(EquivariantMPNNLayer(emb_dim, edge_dim, aggr='add'))

    def forward(self, data):
        h = self.lin_in(data.x)
        for conv in self.convs:
            h = h + conv(h, data.pos, data.edge_index, data.edge_attr)
        h_graph = self.pool(h, data.batch)
        return h_graph

class SiameseEquivariantMPNNModel(nn.Module):
    def __init__(self, num_layers=2, emb_dim=64, in_dim=11, edge_dim=4, dropout=0.5):
        super(SiameseEquivariantMPNNModel, self).__init__()
        self.embedding = EquivariantMPNNModel(num_layers, emb_dim, in_dim, edge_dim)
        self.fc = nn.Sequential(
            nn.Linear(2 * emb_dim, emb_dim),
            nn.ReLU(),
            nn.Dropout(p=dropout),
            nn.Linear(emb_dim, 1)
        )
    def forward(self, data1, data2):
        embedding1 = self.embedding(data1)
        embedding2 = self.embedding(data2)
        combined = torch.cat((embedding1, embedding2), dim=-1)
        similarity = torch.sigmoid(self.fc(combined))
        return similarity.unsqueeze(-1)
