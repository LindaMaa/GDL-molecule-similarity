# -*- coding: utf-8 -*-
"""training-loops.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AoPMtl4QzVzcCUxvn5114nSFUnFCKiZV
"""

def train(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    true_labels = []
    pred_scores = []

    for data1, data2, label in loader:
        data1, data2, label = data1.to(device), data2.to(device), label.to(device)
        optimizer.zero_grad()
        out = model(data1, data2)
        out = out.squeeze(-1) 
        loss = criterion(out, label)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * data1.num_graphs
        true_labels.extend(label.detach().cpu().numpy())
        pred_scores.extend(out.detach().cpu().numpy())

    return total_loss / len(loader.dataset), true_labels, pred_scores


def evaluate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    true_labels = []
    pred_scores = []

    with torch.no_grad():
        for data in loader:
            data1, data2, label = data[0].to(device), data[1].to(device), data[2].to(device)
            out = model(data1, data2)
            out = out.squeeze(-1)
            loss = criterion(out, label)

            total_loss += loss.item()
            true_labels.extend(label.detach().cpu().numpy())
            pred_scores.extend(out.detach().cpu().numpy())

    return total_loss / len(loader.dataset), true_labels, pred_scores


def test(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    true_labels = []
    pred_scores = []

    with torch.no_grad():
        for data in loader:
            data1, data2, label = data[0].to(device), data[1].to(device), data[2].to(device)
            out = model(data1, data2)
            out = out.squeeze(-1)
            pred = (out > 0.5).float()

            correct += (pred == label).sum().item()
            total += label.size(0)

            true_labels.extend(label.detach().cpu().numpy())
            pred_scores.extend(out.detach().cpu().numpy())

    return correct / total, true_labels, pred_scores

def train_early_stopping(model, train_loader, val_loader, criterion, optimizer, device, patience=5):
    best_val_acc = 0
    epochs_without_improvement = 0

    train_losses = []
    val_losses = []
    train_accs=[]
    val_accs = []
    positive_scores = []
    negative_scores = []
    val_Y_true = []
    val_Y_pred = []

    for epoch in range(300):  
        train_loss, train_true_labels, train_pred_scores = train(model, train_loader, criterion, optimizer, device)
        val_loss, val_true_labels, val_pred_scores = evaluate(model, val_loader, criterion, device)
        
        val_acc, val_Y_true_epoch, val_Y_pred_epoch = test(model, val_loader, device)
        train_acc, train_Y_true_epoch, train_Y_pred_epoch = test(model, train_loader, device)

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_accs.append(val_acc)
        train_accs.append(train_acc)

        positive_scores.extend([pred for i, pred in enumerate(val_pred_scores) if val_true_labels[i] == 1])
        negative_scores.extend([pred for i, pred in enumerate(val_pred_scores) if val_true_labels[i] == 0])
        val_Y_true.extend(val_true_labels)
        val_Y_pred.extend(val_pred_scores)

        print(f"Epoch {epoch}, train loss: {train_loss:.4f}, val loss: {val_loss:.4f}, val acc: {val_acc:.4f}")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            epochs_without_improvement = 0
        else:
            epochs_without_improvement += 1
            if epochs_without_improvement >= patience:
                print(f"No improvement for {patience} epochs. Stopping early...")
                break

    return train_losses, val_losses, train_accs, val_accs, positive_scores, negative_scores, val_Y_true, val_Y_pred

# adaptation for CNN
def train(model, dataloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    for batch in dataloader:
        fp1, fp2, labels = [t.to(device) for t in batch]
        optimizer.zero_grad()
        outputs = model(fp1, fp2)
        loss = criterion(outputs, labels.unsqueeze(1))
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        total += labels.size(0)
        correct += ((outputs > 0.5) == labels.unsqueeze(1)).sum().item()

    return running_loss / len(dataloader), correct / total


def evaluate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in dataloader:
            fp1, fp2, labels = [t.to(device) for t in batch]
            outputs = model(fp1, fp2)
            loss = criterion(outputs, labels.unsqueeze(1))

            running_loss += loss.item()
            total += labels.size(0)
            correct += ((outputs > 0.5) == labels.unsqueeze(1)).sum().item()

    return running_loss / len(dataloader), correct / total

def test(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    true_labels = []
    pred_scores = []
    with torch.no_grad():
        for data in loader:
            data1, data2, label = data[0].to(device), data[1].to(device), data[2].to(device)
            out = model(data1, data2)
            out = out.squeeze(-1)
            pred = (out > 0.5).float()
            correct += (pred == label).sum().item()
            total += label.size(0)
            
            true_labels.extend(label.cpu().numpy())
            pred_scores.extend(out.cpu().numpy())

    test_accuracy = correct / total
    return test_accuracy, true_labels, pred_scores


def train_early_stopping(model, train_loader, val_loader, criterion, optimizer, device, patience=5):
    best_val_acc = 0
    epochs_without_improvement = 0
    train_losses = []
    val_losses = []
    train_accs=[]
    val_accs=[]
    val_y_true = []
    val_y_pred = []

    for epoch in range(100):  
        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc = evaluate(model, val_loader, criterion, device)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        print("epoch ", epoch)
        print("train loss ", train_loss)
        print("val loss ", val_loss)
        print("val acc ", val_acc)

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            epochs_without_improvement = 0
        else:
            epochs_without_improvement += 1
            if epochs_without_improvement >= patience:
                break

        model.eval()
        with torch.no_grad():
            for batch in val_loader:
                fp1, fp2, labels = [t.to(device) for t in batch]
                outputs = model(fp1, fp2)
                val_y_true.extend(labels.cpu().numpy())
                val_y_pred.extend(outputs.squeeze().cpu().numpy())

    return train_losses, val_losses,train_accs,val_accs, val_y_true, val_y_pred
