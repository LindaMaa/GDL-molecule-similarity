{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEy4JmnbZYpy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "! pip install rdkit\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install -q rdkit-pypi==2021.9.4\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, ReLU, BatchNorm1d, Module, Sequential\n",
        "from torch_scatter import scatter\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Dataset, Batch\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import (MessagePassing, GATConv, TransformerConv,\n",
        "                                GCNConv, GINConv, ChebConv,\n",
        "                                global_mean_pool, global_add_pool)\n",
        "import rdkit\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import (AllChem, QED, Crippen, rdMolDescriptors,\n",
        "                        rdmolops)\n",
        "from google.colab import files, drive\n",
        "drive.mount('/content/drive')\n",
        "from torch_geometric.nn import TransformerConv\n",
        "from torch_geometric.data import Dataset, Data\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "import random\n",
        "from torch_geometric.data import InMemoryDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = QM9(root='data/QM9')\n",
        "print(f\"Total number of samples: {len(dataset)}.\")"
      ],
      "metadata": {
        "id": "mKkw9hzj5Bwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare dataset - output fingerprints"
      ],
      "metadata": {
        "id": "dYps3lPBaREm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_threshold = 0.7\n",
        "positive_ratio = 0.5\n",
        "\n",
        "train_sim_dataset = QM9MoleculeSimilarityDataset(train_dataset, threshold=similarity_threshold, positive_ratio=positive_ratio)\n",
        "val_sim_dataset = QM9MoleculeSimilarityDataset(val_dataset, threshold=similarity_threshold, positive_ratio=positive_ratio)\n",
        "test_sim_dataset = QM9MoleculeSimilarityDataset(test_dataset, threshold=similarity_threshold, positive_ratio=positive_ratio)\n",
        "\n",
        "train_sim_loader = DataLoader(train_sim_dataset, batch_size=64, shuffle=True,drop_last=True)\n",
        "val_sim_loader = DataLoader(val_sim_dataset, batch_size=64, shuffle=False,drop_last=True)\n",
        "test_sim_loader = DataLoader(test_sim_dataset, batch_size=64, shuffle=False,drop_last=True)"
      ],
      "metadata": {
        "id": "DVxiocDkSSEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_sim_loader:\n",
        "  print(torch.mean(batch[2]))"
      ],
      "metadata": {
        "id": "LrLA1Cv09Nbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models"
      ],
      "metadata": {
        "id": "tJpudlyzaUjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAttention(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.5):\n",
        "        super(GraphAttention, self).__init__()\n",
        "        self.attention = GATConv(in_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.attention(x, edge_index)\n",
        "\n",
        "class GraphAttentionModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
        "        super(GraphAttentionModel, self).__init__()\n",
        "        self.attention1 = GraphAttention(in_channels, hidden_channels, dropout=dropout)\n",
        "        self.attention2 = GraphAttention(hidden_channels, hidden_channels, dropout=dropout)\n",
        "        #self.attention3 = GraphAttention(hidden_channels, out_channels, dropout=dropout)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.attention1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.attention2(x, edge_index)\n",
        "        return global_add_pool(x, data.batch)\n",
        "\n",
        "\n",
        "class SiameseGraphAttentionModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout=0.5):\n",
        "        super(SiameseGraphAttentionModel, self).__init__()\n",
        "        self.embedding = GraphAttentionModel(input_dim, hidden_dim, hidden_dim, dropout=dropout)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data1, data2):\n",
        "        embedding1 = self.embedding(data1)\n",
        "        embedding2 = self.embedding(data2)\n",
        "        combined = torch.cat((embedding1, embedding2), dim=-1)\n",
        "        similarity = self.fc(combined)\n",
        "        return similarity.unsqueeze(-1) \n",
        "\n",
        "class GCNLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.3):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.conv = GCNConv(in_channels, out_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class GraphGCNModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.3):\n",
        "        super(GraphGCNModel, self).__init__()\n",
        "        self.gcn1 = GCNLayer(in_channels, hidden_channels, dropout=dropout)\n",
        "        self.gcn2 = GCNLayer(hidden_channels, out_channels, dropout=dropout)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.gcn1(x, edge_index)\n",
        "        x = self.gcn2(x, edge_index)\n",
        "        return global_mean_pool(x, data.batch)\n",
        "\n",
        "class SiameseGraphGCNModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout=0.3):\n",
        "        super(SiameseGraphGCNModel, self).__init__()\n",
        "        self.embedding = GraphGCNModel(input_dim, hidden_dim, hidden_dim, dropout=dropout)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data1, data2):\n",
        "        embedding1 = self.embedding(data1)\n",
        "        embedding2 = self.embedding(data2)\n",
        "        combined = torch.cat((embedding1, embedding2), dim=-1)\n",
        "        similarity = self.fc(combined)\n",
        "        return similarity.unsqueeze(-1)  # Add an extra dimension at the end\n",
        "\n",
        "class GraphGIN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0):\n",
        "        super(GraphGIN, self).__init__()\n",
        "        self.gin = GINConv(nn.Sequential(nn.Linear(in_channels, out_channels),\n",
        "                                          nn.ReLU(),\n",
        "                                          nn.Linear(out_channels, out_channels)))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.gin(x, edge_index)\n",
        "\n",
        "\n",
        "class GraphGINModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0):\n",
        "        super(GraphGINModel, self).__init__()\n",
        "        self.gin1 = GraphGIN(in_channels, hidden_channels, dropout=dropout)\n",
        "        self.gin2 = GraphGIN(hidden_channels, hidden_channels, dropout=dropout)\n",
        "        #self.gin3 = GraphGIN(hidden_channels, out_channels, dropout=dropout)\n",
        "   \n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.gin1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.gin2(x, edge_index)\n",
        "        return global_mean_pool(x, data.batch)\n",
        "\n",
        "\n",
        "class SiameseGraphGINModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout=0):\n",
        "        super(SiameseGraphGINModel, self).__init__()\n",
        "        self.embedding = GraphGINModel(input_dim, hidden_dim, hidden_dim, dropout=dropout)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data1, data2):\n",
        "        embedding1 = self.embedding(data1)\n",
        "        embedding2 = self.embedding(data2)\n",
        "        combined = torch.cat((embedding1, embedding2), dim=-1)\n",
        "        similarity = torch.sigmoid(self.fc(combined))\n",
        "        return similarity.unsqueeze(-1) \n",
        "\n",
        "class ChebNetLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, K, dropout=0.5):\n",
        "        super(ChebNetLayer, self).__init__()\n",
        "        self.cheb = ChebConv(in_channels, out_channels, K)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.cheb(x, edge_index)\n",
        "\n",
        "class ChebNetModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, K, dropout=0.5):\n",
        "        super(ChebNetModel, self).__init__()\n",
        "        self.cheb1 = ChebNetLayer(in_channels, hidden_channels, K, dropout=dropout)\n",
        "        self.cheb2 = ChebNetLayer(hidden_channels, hidden_channels, K, dropout=dropout)\n",
        "        self.cheb3 = ChebNetLayer(hidden_channels, out_channels, K, dropout=dropout)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.cheb1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.cheb2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.cheb3(x, edge_index)\n",
        "        return global_mean_pool(x, data.batch)\n",
        "\n",
        "class SiameseChebNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, K, dropout=0.5):\n",
        "        super(SiameseChebNetModel, self).__init__()\n",
        "        self.embedding = ChebNetModel(input_dim, hidden_dim, hidden_dim, K, dropout=dropout)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data1, data2):\n",
        "        embedding1 = self.embedding(data1)\n",
        "        embedding2 = self.embedding(data2)\n",
        "        combined = torch.cat((embedding1, embedding2), dim=-1)\n",
        "        similarity = torch.sigmoid(self.fc(combined))\n",
        "        return similarity.unsqueeze(-1) \n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, fingerprint_size, hidden_size, output_size, dropout=0.5):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, hidden_size, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size * (fingerprint_size // 4), output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.cnn(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class SiameseCNNModel(nn.Module):\n",
        "    def __init__(self, fingerprint_size, hidden_size, dropout=0.5):\n",
        "        super(SiameseCNNModel, self).__init__()\n",
        "        self.embedding = CNNModel(fingerprint_size, hidden_size, hidden_size, dropout=dropout)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, fp1, fp2):\n",
        "        embedding1 = self.embedding(fp1)\n",
        "        embedding2 = self.embedding(fp2)\n",
        "        combined = torch.cat((embedding1, embedding2), dim=-1)\n",
        "        similarity = torch.sigmoid(self.fc(combined))\n",
        "        return similarity\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.5):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.transformer = TransformerConv(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            heads=1,\n",
        "            edge_dim=None,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.transformer(x, edge_index)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.transformer1 = TransformerLayer(in_channels, hidden_channels, dropout=dropout)\n",
        "        self.transformer2 = TransformerLayer(hidden_channels, out_channels, dropout=dropout)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.transformer1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.transformer2(x, edge_index)\n",
        "        return global_mean_pool(x, data.batch)\n",
        "\n",
        "\n",
        "class SiameseTransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout=0.5):\n",
        "        super(SiameseTransformerModel, self).__init__()\n",
        "        self.embedding = TransformerModel(input_dim, hidden_dim, hidden_dim, dropout=dropout)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data1, data2):\n",
        "        embedding1 = self.embedding(data1)\n",
        "        embedding2 = self.embedding(data2)\n",
        "        combined = torch.cat((embedding1, embedding2), dim=-1)\n",
        "        similarity = torch.sigmoid(self.fc(combined))\n",
        "        return similarity.unsqueeze(-1)\n",
        "\n",
        "# NOTE THIS CODE IS ADAPTED FROM GDL LABS\n",
        "class MPNNLayer(MessagePassing):\n",
        "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
        "        super().__init__(aggr=aggr)\n",
        "        self.emb_dim = emb_dim\n",
        "        self.edge_dim = edge_dim    \n",
        "        self.mlp_msg = Sequential(\n",
        "            Linear(2*emb_dim + edge_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "        self.mlp_upd = Sequential(\n",
        "            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), \n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "\n",
        "    def forward(self, h, edge_index, edge_attr):\n",
        "        return self.propagate(edge_index, h=h, edge_attr=edge_attr)\n",
        "\n",
        "    def message(self, h_i, h_j, edge_attr):\n",
        "        msg = torch.cat([h_i, h_j, edge_attr], dim=-1)\n",
        "        return self.mlp_msg(msg)\n",
        "    \n",
        "    def aggregate(self, inputs, index):\n",
        "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
        "    \n",
        "    def update(self, aggr_out, h):\n",
        "        upd_out = torch.cat([h, aggr_out], dim=-1)\n",
        "        return self.mlp_upd(upd_out)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')\n",
        "\n",
        "class MPNNModel(Module):\n",
        "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n",
        "\n",
        "        super().__init__()\n",
        "        self.lin_in = Linear(in_dim, emb_dim)\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.convs.append(MPNNLayer(emb_dim, edge_dim, aggr='add'))\n",
        "        self.pool = global_mean_pool\n",
        "        self.lin_pred = Linear(emb_dim, out_dim)\n",
        "        \n",
        "    def forward(self, data):\n",
        "        h = self.lin_in(data.x) \n",
        "        for conv in self.convs:\n",
        "            h = h + conv(h, data.edge_index, data.edge_attr) \n",
        "\n",
        "\n",
        "        h_graph = self.pool(h, data.batch) \n",
        "        out = self.lin_pred(h_graph)\n",
        "        return out.view(-1)\n",
        "\n",
        "class InvariantMPNNLayer(MessagePassing):\n",
        "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
        "        super().__init__(aggr=aggr)\n",
        "        self.emb_dim = emb_dim\n",
        "        self.edge_dim = edge_dim\n",
        "        self.mlp_msg = Sequential(\n",
        "            Linear(2*emb_dim + edge_dim + 1, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "        self.mlp_upd = Sequential(\n",
        "            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), \n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "        \n",
        "\n",
        "    def forward(self, h, pos, edge_index, edge_attr):\n",
        "\n",
        "        return self.propagate(edge_index, h=h,pos=pos,edge_attr=edge_attr)\n",
        "\n",
        "    def message(self, h_i, h_j, pos_i, pos_j, edge_attr):\n",
        "      dist = ((pos_i - pos_j).pow(2).sqrt()) \n",
        "      #print(dist.size())\n",
        "\n",
        "      #print(\"dist after norm\")\n",
        "      dist = torch.norm(dist,dim=-1)\n",
        "      #print(dist.size())\n",
        "      #print(\"dist after reshape\")\n",
        "      dist=dist.reshape(-1,1)\n",
        "      #print(dist.size())\n",
        "      msg = torch.cat([h_i, h_j, edge_attr,dist], dim=-1)\n",
        "      #print(msg.size())\n",
        "      return self.mlp_msg(msg)\n",
        "\n",
        "    \n",
        "    def aggregate(self, inputs, index):\n",
        "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
        "    \n",
        "    def update(self, aggr_out, h):\n",
        "        upd_out = torch.cat([h, aggr_out], dim=-1)\n",
        "        return self.mlp_upd(upd_out)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')\n",
        "\n",
        "\n",
        "class InvariantMPNNModel(MPNNModel):\n",
        "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n",
        "        super().__init__()\n",
        "        self.lin_in = Linear(in_dim, emb_dim)\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.convs.append(InvariantMPNNLayer(emb_dim, edge_dim, aggr='add'))\n",
        "        self.pool = global_mean_pool\n",
        "        self.lin_pred = Linear(emb_dim, out_dim)\n",
        "        \n",
        "    def forward(self, data):\n",
        "        h = self.lin_in(data.x) \n",
        "        for conv in self.convs:\n",
        "            h = h + conv(h, data.pos, data.edge_index, data.edge_attr)  \n",
        "        h_graph = self.pool(h, data.batch) \n",
        "        return h_graph\n",
        "\n",
        "class SiameseInvariantMPNNModel(nn.Module):\n",
        "    def __init__(self, num_layers=2, emb_dim=64, in_dim=11, edge_dim=4, dropout=0.5):\n",
        "        super(SiameseInvariantMPNNModel, self).__init__()\n",
        "        self.embedding = InvariantMPNNModel(num_layers, emb_dim, in_dim, edge_dim)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2 * emb_dim, emb_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(emb_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data1, data2):\n",
        "        embedding1 = self.embedding(data1)\n",
        "        embedding2 = self.embedding(data2)\n",
        "        combined = torch.cat((embedding1, embedding2), dim=-1)\n",
        "        similarity =  torch.sigmoid(self.fc(combined))\n",
        "        return similarity.unsqueeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "class EquivariantMPNNLayer(InvariantMPNNLayer):\n",
        "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
        "        super().__init__(emb_dim=emb_dim, edge_dim=edge_dim, aggr=aggr)\n",
        "\n",
        "    def forward(self, h, pos, edge_index, edge_attr):\n",
        "        out = self.propagate(edge_index, h=h, pos=pos, edge_attr=edge_attr)\n",
        "        return out\n",
        "\n",
        "    def message(self, h_i, h_j, pos_i, pos_j, edge_attr):\n",
        "        dist = (pos_i - pos_j).pow(2).sum(dim=-1).sqrt().unsqueeze(-1)\n",
        "        msg = torch.cat([h_i, h_j, edge_attr, dist], dim=-1)\n",
        "        return self.mlp_msg(msg)\n",
        "\n",
        "    def update(self, aggr_out, h, pos):\n",
        "        upd_out = torch.cat([h, aggr_out], dim=-1)\n",
        "        return self.mlp_upd(upd_out)\n",
        "\n",
        "class EquivariantMPNNModel(InvariantMPNNModel):\n",
        "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n",
        "        super().__init__(num_layers=num_layers, emb_dim=emb_dim, in_dim=in_dim, edge_dim=edge_dim, out_dim=out_dim)\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.convs.append(EquivariantMPNNLayer(emb_dim, edge_dim, aggr='add'))\n",
        "\n",
        "    def forward(self, data):\n",
        "        h = self.lin_in(data.x)\n",
        "        for conv in self.convs:\n",
        "            h = h + conv(h, data.pos, data.edge_index, data.edge_attr)\n",
        "        h_graph = self.pool(h, data.batch)\n",
        "        return h_graph\n",
        "\n",
        "class SiameseEquivariantMPNNModel(nn.Module):\n",
        "    def __init__(self, num_layers=2, emb_dim=64, in_dim=11, edge_dim=4, dropout=0.5):\n",
        "        super(SiameseEquivariantMPNNModel, self).__init__()\n",
        "        self.embedding = EquivariantMPNNModel(num_layers, emb_dim, in_dim, edge_dim)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2 * emb_dim, emb_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(emb_dim, 1)\n",
        "        )\n",
        "    def forward(self, data1, data2):\n",
        "        embedding1 = self.embedding(data1)\n",
        "        embedding2 = self.embedding(data2)\n",
        "        combined = torch.cat((embedding1, embedding2), dim=-1)\n",
        "        similarity = torch.sigmoid(self.fc(combined))\n",
        "        return similarity.unsqueeze(-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "e7atAjrPaT15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    true_labels = []\n",
        "    pred_scores = []\n",
        "\n",
        "    for data1, data2, label in loader:\n",
        "        data1, data2, label = data1.to(device), data2.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data1, data2)\n",
        "        out = out.squeeze(-1)  \n",
        "        loss = criterion(out, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data1.num_graphs\n",
        "        true_labels.extend(label.detach().cpu().numpy())\n",
        "        pred_scores.extend(out.detach().cpu().numpy())\n",
        "\n",
        "    return total_loss / len(loader.dataset), true_labels, pred_scores\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    true_labels = []\n",
        "    pred_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data1, data2, label = data[0].to(device), data[1].to(device), data[2].to(device)\n",
        "            out = model(data1, data2)\n",
        "            out = out.squeeze(-1)\n",
        "            loss = criterion(out, label)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            true_labels.extend(label.detach().cpu().numpy())\n",
        "            pred_scores.extend(out.detach().cpu().numpy())\n",
        "\n",
        "    return total_loss / len(loader.dataset), true_labels, pred_scores\n",
        "\n",
        "\n",
        "def test(model, loader, device):\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    pred_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data1, data2, label = data[0].to(device), data[1].to(device), data[2].to(device)\n",
        "            out = model(data1, data2)\n",
        "            out = out.squeeze(-1)\n",
        "\n",
        "            true_labels.extend(label.detach().cpu().numpy())\n",
        "            pred_scores.extend(out.detach().cpu().numpy())\n",
        "      \n",
        " \n",
        "    mse = mean_squared_error(true_labels, pred_scores)\n",
        "\n",
        "    return mse, true_labels, pred_scores\n",
        "\n",
        "def train_early_stopping(model, train_loader, val_loader, criterion, optimizer, device, patience=5):\n",
        "    best_val_mse = 0\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_mses=[]\n",
        "    val_mses = []\n",
        "\n",
        "\n",
        "    for epoch in range(300): \n",
        "        train_loss, train_true_labels, train_pred_scores = train(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_true_labels, val_pred_scores = evaluate(model, val_loader, criterion, device)\n",
        "        \n",
        "        val_mse, val_Y_true_epoch, val_Y_pred_epoch = test(model, val_loader, device)\n",
        "        train_mse, train_Y_true_epoch, train_Y_pred_epoch = test(model, train_loader, device)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_mses.append(val_mse)\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        print(f\"Epoch {epoch}, train loss: {train_loss:.4f}, val loss: {val_loss:.4f}, val mse: {val_mse:.4f}\")\n",
        "\n",
        "        if val_mse < best_val_mse:\n",
        "            best_val_mse = val_mse\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(f\"No improvement for {patience} epochs. Stopping early...\")\n",
        "                break\n",
        "\n",
        "    return train_losses, val_losses, train_mses, val_mses\n"
      ],
      "metadata": {
        "id": "K7iCsHp8b-Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN GAT"
      ],
      "metadata": {
        "id": "UGmN6tQscI7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "in_channels = 11 \n",
        "model = SiameseGraphAttentionModel(input_dim=11, hidden_dim=64,dropout=0).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "train_losses, val_losses, train_mse,val_mse= train_early_stopping(model, train_sim_loader, val_sim_loader, criterion, optimizer, device, patience=20)\n",
        "test_mse,true_labels, pred_scores = test(model, test_sim_loader, device)\n",
        "print(\"test mse\", test_mse)"
      ],
      "metadata": {
        "id": "I6voNygkcJqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('GAT loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_mse, label='Training MSE')\n",
        "plt.plot(val_mse, label='Validation MSE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.title('GAT MSE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BsAcw4r_fOMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN GCN"
      ],
      "metadata": {
        "id": "1S3pydJjfhQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "in_channels = 11 \n",
        "model = SiameseGraphGCNModel(input_dim=11, hidden_dim=64,dropout=0).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "train_losses, val_losses, train_mse,val_mse= train_early_stopping(model, train_sim_loader, val_sim_loader, criterion, optimizer, device, patience=20)\n",
        "test_mse,true_labels, pred_scores = test(model, test_sim_loader, device)\n",
        "print(\"test mse\", test_mse)"
      ],
      "metadata": {
        "id": "VnwY92MlbPXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('GCN loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_mse, label='Training MSE')\n",
        "plt.plot(val_mse, label='Validation MSE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.title('GCN MSE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mpQN7Z0wk17w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN GIN"
      ],
      "metadata": {
        "id": "uxbsWK-PpQr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "in_channels = 11 \n",
        "model = SiameseGraphGINModel(input_dim=11, hidden_dim=64,dropout=0).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "train_losses, val_losses, train_mse,val_mse= train_early_stopping(model, train_sim_loader, val_sim_loader, criterion, optimizer, device, patience=20)\n",
        "test_mse,true_labels, pred_scores = test(model, test_sim_loader, device)\n",
        "print(\"test mse\", test_mse)"
      ],
      "metadata": {
        "id": "n5gynF_Mo9Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('GIN loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_mse, label='Training MSE')\n",
        "plt.plot(val_mse, label='Validation MSE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.title('GIN MSE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oc84IscqsCbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN CHEBNET"
      ],
      "metadata": {
        "id": "N3tY-RU3sPa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "in_channels = 11 \n",
        "model = SiameseChebNetModel(input_dim=11, hidden_dim=64,K=4,dropout=0).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "train_losses, val_losses, train_mse,val_mse= train_early_stopping(model, train_sim_loader, val_sim_loader, criterion, optimizer, device, patience=20)\n",
        "test_mse,true_labels, pred_scores = test(model, test_sim_loader, device)\n",
        "print(\"test mse\", test_mse)"
      ],
      "metadata": {
        "id": "2Wy-O01QefOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('ChebNet loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_mse, label='Training MSE')\n",
        "plt.plot(val_mse, label='Validation MSE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.title('ChebNet MSE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ou02wS2zuo1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN TRANSFORMER"
      ],
      "metadata": {
        "id": "s8gTdHcmfISM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "in_channels = 11 \n",
        "model = SiameseTransformerModel(input_dim=11, hidden_dim=128,dropout=0).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "train_losses, val_losses, train_mse,val_mse= train_early_stopping(model, train_sim_loader, val_sim_loader, criterion, optimizer, device, patience=20)\n",
        "test_mse,true_labels, pred_scores = test(model, test_sim_loader, device)\n",
        "print(\"test mse\", test_mse)"
      ],
      "metadata": {
        "id": "lSDvnNEqfHVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Transformer loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_mse, label='Training MSE')\n",
        "plt.plot(val_mse, label='Validation MSE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.title('Transformer MSE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QY0UFEcihaxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN CNN"
      ],
      "metadata": {
        "id": "-p3OKd3jh5-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QM9MoleculeSimilarityDataset(InMemoryDataset):\n",
        "    def __init__(self, dataset, threshold=0.7, positive_ratio=0.5):\n",
        "        self.dataset = dataset\n",
        "        self.threshold = threshold\n",
        "        self.positive_ratio = positive_ratio\n",
        "        super(QM9MoleculeSimilarityDataset, self).__init__(dataset.root)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['processed_mol_similarity_data.pt']\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        data_list = []\n",
        "        for i in range(len(self.dataset)):\n",
        "            data1, data2, label = self.get(i)\n",
        "            data = Data(data1=data1, data2=data2, y=label)\n",
        "            data_list.append(data)\n",
        "\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def _get_fingerprint(self, mol):\n",
        "        return rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
        "\n",
        "    def _compute_similarity(self, mol1, mol2):\n",
        "        fp1 = self._get_fingerprint(mol1)\n",
        "        fp2 = self._get_fingerprint(mol2)\n",
        "        return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
        "\n",
        "    def _get_positive_pair(self, index):\n",
        "      data1 = self.dataset[index]\n",
        "      mol1 = self._get_rdkit_mol(data1)\n",
        "      for _ in range(30): \n",
        "          other_index = random.randint(0, len(self.dataset) - 1)\n",
        "          data2 = self.dataset[other_index]\n",
        "          mol2 = self._get_rdkit_mol(data2)\n",
        "          similarity = self._compute_similarity(mol1, mol2)\n",
        "          if similarity >= self.threshold:\n",
        "              return self.dataset[index], self.dataset[other_index], torch.tensor([similarity], dtype=torch.float)\n",
        "      return None, None, None\n",
        "\n",
        "    def _get_negative_pair(self, index):\n",
        "        data1 = self.dataset[index]\n",
        "        mol1 = self._get_rdkit_mol(data1)\n",
        "        for _ in range(30): \n",
        "            other_index = random.randint(0, len(self.dataset) - 1)\n",
        "            data2 = self.dataset[other_index]\n",
        "            mol2 = self._get_rdkit_mol(data2)\n",
        "            similarity = self._compute_similarity(mol1, mol2)\n",
        "            if similarity < self.threshold:\n",
        "                return self.dataset[index], self.dataset[other_index], torch.tensor([similarity], dtype=torch.float)\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "\n",
        "    def _get_rdkit_mol(self, data):\n",
        "      atomic_numbers = data.z.tolist()\n",
        "      positions = data.pos.tolist()\n",
        "\n",
        "      mol = Chem.EditableMol(Chem.Mol())\n",
        "      for z in atomic_numbers:\n",
        "          mol.AddAtom(Chem.Atom(int(z)))\n",
        "\n",
        "      mol = mol.GetMol()\n",
        "\n",
        "      conf = Chem.Conformer(len(atomic_numbers))\n",
        "      for i, pos in enumerate(positions):\n",
        "          conf.SetAtomPosition(i, pos)\n",
        "      mol.AddConformer(conf)\n",
        "      Chem.SanitizeMol(mol)\n",
        "\n",
        "      return mol\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _compute_similarity(self, mol1, mol2):\n",
        "      fp1 = self._get_fingerprint(mol1)\n",
        "      fp2 = self._get_fingerprint(mol2)\n",
        "      similarity = DataStructs.FingerprintSimilarity(fp1, fp2)\n",
        "      return similarity\n",
        "\n",
        "\n",
        "    def get(self, index):\n",
        "        is_positive = random.random() < self.positive_ratio\n",
        "        if is_positive:\n",
        "            data1, data2, label = self._get_positive_pair(index)\n",
        "            if data1 is not None and data2 is not None:\n",
        "                return data1, data2, label\n",
        "        else:\n",
        "            data1, data2, label = self._get_negative_pair(index)\n",
        "            if data1 is not None and data2 is not None:\n",
        "                return data1, data2, label\n",
        "\n",
        "      \n",
        "        other_index = random.randint(0, len(self.dataset) - 1)\n",
        "        return self.dataset[index], self.dataset[other_index], torch.tensor([0.0], dtype=torch.float)\n",
        "\n",
        "class FingerprintSimilarityDataset(QM9MoleculeSimilarityDataset):\n",
        "    def __getitem__(self, index):\n",
        "        data1, data2, label = super(FingerprintSimilarityDataset, self).get(index)\n",
        "        mol1 = self._get_rdkit_mol(data1)\n",
        "        mol2 = self._get_rdkit_mol(data2)\n",
        "\n",
        "        fp1 = torch.tensor(self._get_fingerprint(mol1), dtype=torch.float)\n",
        "        fp2 = torch.tensor(self._get_fingerprint(mol2), dtype=torch.float)\n",
        "\n",
        "        return fp1, fp2, label\n",
        "\n",
        "train_fp_dataset = FingerprintSimilarityDataset(train_dataset, threshold=0.7, positive_ratio=0.5)\n",
        "val_fp_dataset = FingerprintSimilarityDataset(val_dataset, threshold=0.7, positive_ratio=0.5)\n",
        "test_fp_dataset = FingerprintSimilarityDataset(test_dataset, threshold=0.7, positive_ratio=0.5)\n",
        "\n",
        "train_fp_loader = DataLoader(train_fp_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "val_fp_loader = DataLoader(val_fp_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
        "test_fp_loader = DataLoader(test_fp_dataset, batch_size=32, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "GgZg8_I5h6lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    for batch in dataloader:\n",
        "        fp1, fp2, labels = [t.to(device) for t in batch]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(fp1, fp2)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * fp1.size(0)\n",
        "        total += fp1.size(0)\n",
        "\n",
        "    return running_loss / total\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            fp1, fp2, labels = [t.to(device) for t in batch]\n",
        "            outputs = model(fp1, fp2)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * fp1.size(0)\n",
        "            total += fp1.size(0)\n",
        "\n",
        "    return running_loss / total\n",
        "\n",
        "\n",
        "def test(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    true_labels = []\n",
        "    pred_scores = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data1, data2, label = data[0].to(device), data[1].to(device), data[2].to(device)\n",
        "            out = model(data1, data2)\n",
        "            loss = criterion(out, label)\n",
        "            total_loss += loss.item() * data1.size(0)\n",
        "            true_labels.extend(label.cpu().numpy())\n",
        "            pred_scores.extend(out.cpu().numpy())\n",
        "\n",
        "    test_mse = total_loss / len(loader.dataset)\n",
        "    return test_mse, true_labels, pred_scores\n",
        "\n",
        "\n",
        "def train_early_stopping(model, train_loader, val_loader, criterion, optimizer, device, patience=5):\n",
        "    best_val_mse = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_mses=[]\n",
        "    val_mses=[]\n",
        "    val_y_true = []\n",
        "    val_y_pred = []\n",
        "\n",
        "    for epoch in range(100):  \n",
        "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss = evaluate(model, val_loader, criterion, device)\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(\"epoch \", epoch)\n",
        "        print(\"train loss \", train_loss)\n",
        "        print(\"val loss \", val_loss)\n",
        "        train_mses.append(test(model, train_loader, criterion, device))\n",
        "        val_mses.append(test(model, val_loader, criterion, device))\n",
        "\n",
        "        if val_loss < best_val_mse:\n",
        "            best_val_mse = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "            test_mse, true_labels, pred_scores = test(model, val_loader, criterion, device)\n",
        "            val_y_true = true_labels\n",
        "            val_y_pred = pred_scores\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                break\n",
        "\n",
        "    return train_losses, val_losses,train_mses,val_mses, val_y_true, val_y_pred, test_mse\n"
      ],
      "metadata": {
        "id": "wsDhvRDJkarS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "in_channels = 11 \n",
        "fingerprint_size=2048\n",
        "model = SiameseCNNModel(fingerprint_size, 128).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "train_losses, val_losses,train_mses,val_mses, val_y_true, val_y_pred, test_mse= train_early_stopping(model, train_fp_loader, val_fp_loader, criterion, optimizer, device, patience=20)\n",
        "test_mse,true_labels, pred_scores = test(model, test_fp_loader, criterion, device)\n",
        "print(\"test mse\", test_mse)"
      ],
      "metadata": {
        "id": "Wgx5-LxrjH8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "in_channels = 11 \n",
        "model = SiameseInvariantMPNNModel(num_layers=2, emb_dim=128, in_dim=11, edge_dim=4, dropout=0).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "train_losses, val_losses, train_mse,val_mse= train_early_stopping(model, train_sim_loader, val_sim_loader, criterion, optimizer, device, patience=20)\n",
        "test_mse,true_labels, pred_scores = test(model, test_sim_loader, device)\n",
        "print(\"test mse\", test_mse)"
      ],
      "metadata": {
        "id": "rxtDC8ao6dHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Invariant MPNN loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_mse, label='Training MSE')\n",
        "plt.plot(val_mse, label='Validation MSE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.title('Invariant MPNN MSE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z-0qMSRP7sDu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
